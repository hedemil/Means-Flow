{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeanFlow Training Diagnostics\n",
    "\n",
    "This notebook helps diagnose why training isn't improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Prevent JAX from preallocating all GPU memory\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/emil/KTH/Adv. Deep Learning/Project/Means Flow/src')\n",
    "\n",
    "from data.cifar10 import make_cifar10\n",
    "from core.schedules import linear_path, sample_r_t\n",
    "from models.meanflow_net import MeanFlowNet\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Check if model can overfit to a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch\n",
    "ds = make_cifar10(batch_size=8, split=\"train[:100]\", shuffle=False, cache=True)\n",
    "single_batch = next(iter(ds.as_numpy_iterator()))\n",
    "images, labels = single_batch\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "\n",
    "# Store a copy to avoid accidental overwriting\n",
    "images_test = images.copy()\n",
    "labels_test = labels.copy()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.clip(images[i], 0, 1))\n",
    "    ax.set_title(f\"Class {labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Verify time sampling and noising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test time sampling\n",
    "rng = jax.random.PRNGKey(42)\n",
    "r, t = sample_r_t(rng, batch=8)\n",
    "\n",
    "print(\"Time samples:\")\n",
    "for i in range(8):\n",
    "    print(f\"  r={r[i]:.3f}, t={t[i]:.3f}, t-r={t[i]-r[i]:.3f}\")\n",
    "\n",
    "# Verify r < t always\n",
    "assert jnp.all(r <= t), \"r should always be <= t\"\n",
    "print(\"\\n✓ Time sampling constraint satisfied: r <= t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test linear path noising\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, noise_rng, time_rng = jax.random.split(rng, 3)\n",
    "\n",
    "# Get one image\n",
    "x_clean = images[0:1]  # [1, 32, 32, 3]\n",
    "eps = jax.random.normal(noise_rng, x_clean.shape)\n",
    "\n",
    "# Test at different time steps\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "test_times = jnp.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "for i, t_val in enumerate(test_times):\n",
    "    t = jnp.array([t_val])\n",
    "    zt, v_t = linear_path(x_clean, eps, t)\n",
    "    \n",
    "    # Plot noisy image\n",
    "    axes[0, i].imshow(np.clip(zt[0], 0, 1))\n",
    "    axes[0, i].set_title(f't={t_val:.2f}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot velocity field (as RGB)\n",
    "    v_vis = (v_t[0] - v_t[0].min()) / (v_t[0].max() - v_t[0].min() + 1e-8)\n",
    "    axes[1, i].imshow(v_vis)\n",
    "    axes[1, i].set_title(f'velocity')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Noisy Image', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Velocity', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Linear path interpolates correctly from clean (t=0) to noise (t=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Check model initialization and forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model - FRESH INITIALIZATION (no cached params)\n",
    "# If you get shape errors, make sure to run this cell fresh after any model changes!\n",
    "\n",
    "# Use the test batch (ensure correct size)\n",
    "test_images = images_test[:8]  # Force batch size 8\n",
    "test_labels = labels_test[:8]\n",
    "\n",
    "print(f\"DEBUG: test_images.shape = {test_images.shape}\")\n",
    "print(f\"DEBUG: test_labels.shape = {test_labels.shape}\")\n",
    "print(f\"DEBUG: test_labels = {test_labels}\")\n",
    "print()\n",
    "\n",
    "model = MeanFlowNet(\n",
    "    in_ch=3,\n",
    "    latent_hw=32,\n",
    "    ch=32,\n",
    "    num_classes=10,\n",
    "    ch_mult=(1, 2, 4),\n",
    "    num_res_blocks=2\n",
    ")\n",
    "\n",
    "# Use a fresh RNG key\n",
    "rng_model = jax.random.PRNGKey(999)  # Changed seed to force fresh init\n",
    "rng_model, init_rng = jax.random.split(rng_model)\n",
    "\n",
    "# Dummy inputs\n",
    "dummy_x = jnp.zeros((8, 32, 32, 3))\n",
    "dummy_r = jnp.zeros((8,))\n",
    "dummy_t = jnp.ones((8,))\n",
    "dummy_cls = jnp.zeros((8,), dtype=jnp.int32)\n",
    "\n",
    "print(\"Initializing model with fresh parameters...\")\n",
    "params = model.init(init_rng, dummy_x, dummy_r, dummy_t, dummy_cls,\n",
    "                   train_cfg_drop=0.1, rng=init_rng)[\"params\"]\n",
    "print(\"Model initialized successfully!\")\n",
    "\n",
    "# Test forward pass with fresh r, t variables (avoid variable pollution from previous cells)\n",
    "rng_model, forward_rng, time_rng = jax.random.split(rng_model, 3)\n",
    "r_test, t_test = sample_r_t(time_rng, batch=8)\n",
    "\n",
    "output = model.apply({\"params\": params}, test_images, r_test, t_test, test_labels.astype(jnp.int32),\n",
    "                    train_cfg_drop=0.1, rng=forward_rng)\n",
    "\n",
    "print(f\"\\nInput shape: {test_images.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "print(f\"Output mean: {output.mean():.6f}\")\n",
    "print(f\"Output std: {output.std():.6f}\")\n",
    "\n",
    "# Check if output is all zeros (bad initialization)\n",
    "if jnp.abs(output).max() < 1e-5:\n",
    "    print(\"\\n⚠️  WARNING: Output is nearly zero! Model might not be initialized properly.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Model produces non-zero outputs\")\n",
    "    print(f\"  Max abs output: {jnp.abs(output).max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Check loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.identity import meanflow_target\n",
    "\n",
    "# Sample some noisy data with fresh variables\n",
    "rng_loss = jax.random.PRNGKey(42)\n",
    "rng_loss, noise_rng, time_rng = jax.random.split(rng_loss, 3)\n",
    "\n",
    "eps = jax.random.normal(noise_rng, images.shape)\n",
    "t_sample = jax.random.uniform(time_rng, (8,))\n",
    "zt, v_t = linear_path(images, eps, t_sample)\n",
    "r_loss, t_loss = sample_r_t(time_rng, 8)\n",
    "\n",
    "# Define u_apply\n",
    "def u_apply(params, zt_, r_, t_, cls_idx_, rng_local):\n",
    "    return model.apply({\"params\": params}, zt_, r_, t_, cls_idx_,\n",
    "                      train_cfg_drop=0.1, rng=rng_local)\n",
    "\n",
    "# Compute target\n",
    "rng_loss, target_rng = jax.random.split(rng_loss)\n",
    "u_pred, u_star = meanflow_target(u_apply, params, zt, r_loss, t_loss, \n",
    "                                labels.astype(jnp.int32), v_t, rng=target_rng)\n",
    "\n",
    "loss = jnp.mean((u_pred - u_star)**2)\n",
    "\n",
    "print(\"Loss computation test:\")\n",
    "print(f\"  u_pred shape: {u_pred.shape}\")\n",
    "print(f\"  u_star shape: {u_star.shape}\")\n",
    "print(f\"  u_pred range: [{u_pred.min():.3f}, {u_pred.max():.3f}]\")\n",
    "print(f\"  u_star range: [{u_star.min():.3f}, {u_star.max():.3f}]\")\n",
    "print(f\"  Loss: {loss:.6f}\")\n",
    "\n",
    "if loss > 10.0:\n",
    "    print(\"\\n⚠️  WARNING: Initial loss is very high. This might make learning difficult.\")\n",
    "elif loss < 0.01:\n",
    "    print(\"\\n⚠️  WARNING: Initial loss is very low. Check if target is being computed correctly.\")\n",
    "else:\n",
    "    print(\"\\n✓ Loss is in a reasonable range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Overfit on single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with higher learning rate for overfitting test\n",
    "tx = optax.adam(learning_rate=1e-3)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=tx\n",
    ")\n",
    "\n",
    "@jax.jit\n",
    "def train_step_simple(state, batch, rng):\n",
    "    images, labels = batch\n",
    "    B = images.shape[0]\n",
    "    \n",
    "    rng, rng_r_t, rng_eps, rng_drop = jax.random.split(rng, 4)\n",
    "    \n",
    "    eps = jax.random.normal(rng_eps, images.shape)\n",
    "    zt, v_t = linear_path(images, eps, jax.random.uniform(rng_r_t, (B,)))\n",
    "    r, t = sample_r_t(rng_r_t, B)\n",
    "    cls_idx = labels.astype(jnp.int32)\n",
    "    \n",
    "    def u_apply(params, zt_, r_, t_, cls_idx_, rng_local):\n",
    "        return state.apply_fn({\"params\": params}, zt_, r_, t_, cls_idx_,\n",
    "                             train_cfg_drop=0.1, rng=rng_local)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        u_pred, u_star = meanflow_target(u_apply, params, zt, r, t, cls_idx, v_t, rng=rng_drop)\n",
    "        return jnp.mean((u_pred - u_star)**2)\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    # Check gradient norms\n",
    "    grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_util.tree_leaves(grads)))\n",
    "    \n",
    "    return new_state, {\"loss\": loss, \"grad_norm\": grad_norm}, rng\n",
    "\n",
    "print(\"Overfitting test on single batch...\")\n",
    "print(\"If the model can learn, loss should decrease dramatically.\\n\")\n",
    "\n",
    "losses = []\n",
    "grad_norms = []\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "for step in range(200):\n",
    "    state, metrics, rng = train_step_simple(state, single_batch, rng)\n",
    "    losses.append(float(metrics[\"loss\"]))\n",
    "    grad_norms.append(float(metrics[\"grad_norm\"]))\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step:3d}: loss={metrics['loss']:.6f}, grad_norm={metrics['grad_norm']:.3f}\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss on Single Batch (Overfitting Test)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(grad_norms)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Gradient Norm')\n",
    "ax2.set_title('Gradient Norms')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnosis\n",
    "initial_loss = losses[0]\n",
    "final_loss = losses[-1]\n",
    "improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DIAGNOSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Initial loss: {initial_loss:.6f}\")\n",
    "print(f\"Final loss: {final_loss:.6f}\")\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "\n",
    "if improvement > 50:\n",
    "    print(\"\\n✓ GOOD: Model can learn! Loss decreased significantly.\")\n",
    "    print(\"   Issue is likely: learning rate too small, need more training, or dataset size.\")\n",
    "elif improvement > 10:\n",
    "    print(\"\\n⚠️  MODERATE: Model is learning slowly.\")\n",
    "    print(\"   Suggestions: increase learning rate, simplify model, or check data preprocessing.\")\n",
    "else:\n",
    "    print(\"\\n❌ PROBLEM: Model is not learning!\")\n",
    "    print(\"   Possible issues:\")\n",
    "    print(\"   - Loss function implementation\")\n",
    "    print(\"   - Model architecture bugs\")\n",
    "    print(\"   - Gradient flow problems\")\n",
    "    print(\"   - Data preprocessing issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Visualize what model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into what the model is actually learning\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED LOSS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a fresh batch and compute everything step by step\n",
    "rng_debug = jax.random.PRNGKey(999)\n",
    "test_batch = single_batch\n",
    "test_images, test_labels = test_batch\n",
    "B = test_images.shape[0]\n",
    "\n",
    "# Sample random components\n",
    "rng_debug, rng_r_t, rng_eps = jax.random.split(rng_debug, 3)\n",
    "eps = jax.random.normal(rng_eps, test_images.shape)\n",
    "r, t = sample_r_t(rng_r_t, B)\n",
    "\n",
    "print(f\"\\n1. Input data:\")\n",
    "print(f\"   Images shape: {test_images.shape}\")\n",
    "print(f\"   Images range: [{test_images.min():.3f}, {test_images.max():.3f}]\")\n",
    "print(f\"   Labels: {test_labels}\")\n",
    "\n",
    "print(f\"\\n2. Time sampling:\")\n",
    "print(f\"   r range: [{r.min():.3f}, {r.max():.3f}]\")\n",
    "print(f\"   t range: [{t.min():.3f}, {t.max():.3f}]\")\n",
    "print(f\"   t-r range: [{(t-r).min():.3f}, {(t-r).max():.3f}]\")\n",
    "\n",
    "# Create noisy images\n",
    "zt, v_t = linear_path(test_images, eps, t)\n",
    "print(f\"\\n3. Noisy images:\")\n",
    "print(f\"   zt range: [{zt.min():.3f}, {zt.max():.3f}]\")\n",
    "print(f\"   v_t (true velocity) range: [{v_t.min():.3f}, {v_t.max():.3f}]\")\n",
    "print(f\"   v_t mean: {v_t.mean():.6f}, std: {v_t.std():.6f}\")\n",
    "\n",
    "# Get model prediction\n",
    "rng_debug, pred_rng = jax.random.split(rng_debug)\n",
    "u_pred_direct = model.apply(\n",
    "    {\"params\": state.params}, \n",
    "    zt, r, t, test_labels.astype(jnp.int32),\n",
    "    train_cfg_drop=0.0, \n",
    "    rng=pred_rng\n",
    ")\n",
    "\n",
    "print(f\"\\n4. Model prediction (u_pred):\")\n",
    "print(f\"   u_pred range: [{u_pred_direct.min():.3f}, {u_pred_direct.max():.3f}]\")\n",
    "print(f\"   u_pred mean: {u_pred_direct.mean():.6f}, std: {u_pred_direct.std():.6f}\")\n",
    "\n",
    "# Compute the meanflow target\n",
    "def u_apply_debug(params, zt_, r_, t_, cls_idx_, rng_local):\n",
    "    return model.apply({\"params\": params}, zt_, r_, t_, cls_idx_,\n",
    "                      train_cfg_drop=0.0, rng=rng_local)\n",
    "\n",
    "from core.identity import meanflow_target\n",
    "rng_debug, target_rng = jax.random.split(rng_debug)\n",
    "u_pred, u_star = meanflow_target(\n",
    "    u_apply_debug, state.params, zt, r, t, \n",
    "    test_labels.astype(jnp.int32), v_t, rng=target_rng\n",
    ")\n",
    "\n",
    "print(f\"\\n5. MeanFlow target computation:\")\n",
    "print(f\"   u_pred range: [{u_pred.min():.3f}, {u_pred.max():.3f}]\")\n",
    "print(f\"   u_star (target) range: [{u_star.min():.3f}, {u_star.max():.3f}]\")\n",
    "print(f\"   u_star mean: {u_star.mean():.6f}, std: {u_star.std():.6f}\")\n",
    "\n",
    "# Compute loss\n",
    "mse_loss = jnp.mean((u_pred - u_star)**2)\n",
    "print(f\"\\n6. Loss:\")\n",
    "print(f\"   MSE loss: {mse_loss:.6f}\")\n",
    "\n",
    "# Check if predictions are reasonable\n",
    "diff = u_pred - u_star\n",
    "print(f\"\\n7. Prediction error analysis:\")\n",
    "print(f\"   Error (u_pred - u_star) range: [{diff.min():.3f}, {diff.max():.3f}]\")\n",
    "print(f\"   Error mean: {diff.mean():.6f}, std: {diff.std():.6f}\")\n",
    "print(f\"   Relative error: {(jnp.abs(diff).mean() / jnp.abs(u_star).mean()):.2%}\")\n",
    "\n",
    "# Check if model output correlates with target at all\n",
    "correlation = jnp.corrcoef(u_pred.flatten(), u_star.flatten())[0, 1]\n",
    "print(f\"   Correlation between u_pred and u_star: {correlation:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if jnp.abs(u_pred).max() < 1e-4:\n",
    "    print(\"❌ CRITICAL: Model outputs are nearly zero!\")\n",
    "    print(\"   → Check model initialization\")\n",
    "elif jnp.abs(correlation) < 0.1:\n",
    "    print(\"❌ CRITICAL: No correlation between prediction and target!\")\n",
    "    print(\"   → Model is not learning the right thing\")\n",
    "elif mse_loss > 1.0:\n",
    "    print(\"⚠️  WARNING: Loss is very high\")\n",
    "    print(\"   → Model needs more training or higher learning rate\")\n",
    "elif mse_loss < 0.01:\n",
    "    print(\"✓ EXCELLENT: Model is predicting well!\")\n",
    "else:\n",
    "    print(\"⚠️  MODERATE: Model is learning but slowly\")\n",
    "    print(f\"   → Loss {mse_loss:.4f} should decrease with more training\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5.5: Deep Dive into Loss Computation\n",
    "\n",
    "Let's check what's actually happening in the loss computation step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if model output changes with different t values\n",
    "print(\"Testing if model responds to time conditioning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "x_test = test_images[0:1]  # Single image\n",
    "cls_test = test_labels[0:1].astype(jnp.int32)\n",
    "r_fixed = jnp.array([0.0])  # Fixed r\n",
    "\n",
    "# Test at different time points\n",
    "test_t_values = jnp.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "outputs_at_different_t = []\n",
    "\n",
    "for t_val in test_t_values:\n",
    "    t_test = jnp.array([t_val])\n",
    "    output = model.apply(\n",
    "        {\"params\": state.params},\n",
    "        x_test, r_fixed, t_test, cls_test,\n",
    "        train_cfg_drop=0.0,\n",
    "        rng=None\n",
    "    )\n",
    "    outputs_at_different_t.append(output)\n",
    "    print(f\"t={t_val:.2f}: output mean={output.mean():.6f}, std={output.std():.6f}\")\n",
    "\n",
    "# Check if outputs are different\n",
    "print(f\"\\nVariance across time steps:\")\n",
    "for i in range(1, len(outputs_at_different_t)):\n",
    "    diff = outputs_at_different_t[i] - outputs_at_different_t[0]\n",
    "    print(f\"  ||output(t={test_t_values[i]:.2f}) - output(t=0.0)||² = {jnp.mean(diff**2):.6f}\")\n",
    "\n",
    "# If all differences are near zero, model is ignoring time!\n",
    "max_diff = max([jnp.mean((outputs_at_different_t[i] - outputs_at_different_t[0])**2) \n",
    "                for i in range(1, len(outputs_at_different_t))])\n",
    "\n",
    "print(f\"\\nMax difference from t=0.0: {max_diff:.6f}\")\n",
    "\n",
    "if max_diff < 1e-6:\n",
    "    print(\"❌ CRITICAL BUG: Model output doesn't change with time!\")\n",
    "    print(\"   → Time embedding is not working\")\n",
    "elif max_diff < 0.01:\n",
    "    print(\"⚠️  WARNING: Model barely responds to time changes\")\n",
    "    print(\"   → Time conditioning might be too weak\")\n",
    "else:\n",
    "    print(\"✓ Model responds to time conditioning\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5.6: Check Time Conditioning\n",
    "\n",
    "Verify that the model actually responds to different time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions at different noise levels\n",
    "rng_viz = jax.random.PRNGKey(42)\n",
    "x_clean = images[0:1]\n",
    "cls = labels[0:1].astype(jnp.int32)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "test_times = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "rng_viz, noise_rng = jax.random.split(rng_viz)\n",
    "eps = jax.random.normal(noise_rng, x_clean.shape)\n",
    "\n",
    "for i, t_val in enumerate(test_times):\n",
    "    t_viz = jnp.array([t_val])\n",
    "    r_viz = jnp.array([max(0.0, t_val - 0.1)])  # r slightly less than t\n",
    "    \n",
    "    zt, v_t = linear_path(x_clean, eps, t_viz)\n",
    "    \n",
    "    rng_viz, pred_rng = jax.random.split(rng_viz)\n",
    "    u_pred = model.apply({\"params\": state.params}, zt, r_viz, t_viz, cls,\n",
    "                        train_cfg_drop=0.0, rng=pred_rng)\n",
    "    \n",
    "    # Visualize\n",
    "    axes[0, i].imshow(np.clip(zt[0], 0, 1))\n",
    "    axes[0, i].set_title(f't={t_val:.2f}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Velocity target (normalized for visualization)\n",
    "    v_vis = (v_t[0] - v_t[0].min()) / (v_t[0].max() - v_t[0].min() + 1e-8)\n",
    "    axes[1, i].imshow(v_vis)\n",
    "    axes[1, i].set_title('Target velocity')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Predicted velocity (normalized for visualization)\n",
    "    u_vis = (u_pred[0] - u_pred[0].min()) / (u_pred[0].max() - u_pred[0].min() + 1e-8)\n",
    "    axes[2, i].imshow(u_vis)\n",
    "    axes[2, i].set_title('Predicted velocity')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Noisy Input', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Target', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Prediction', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Check if predictions look similar to targets (especially after overfitting test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the tests above, we can diagnose the training issues:\n",
    "\n",
    "1. **If overfitting test shows improvement**: The model CAN learn, but needs:\n",
    "   - Higher learning rate (try 3e-4 or 5e-4)\n",
    "   - Fewer images initially (start with 1000-5000)\n",
    "   - More training steps\n",
    "\n",
    "2. **If overfitting test shows NO improvement**: There's a bug in:\n",
    "   - Loss computation\n",
    "   - Model architecture\n",
    "   - Data preprocessing\n",
    "\n",
    "3. **If gradients are vanishing** (grad_norm < 0.01): Check:\n",
    "   - Model initialization\n",
    "   - Activation functions\n",
    "   - Gradient flow through architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meanflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
