data:
  name: cifar10
  batch_size: 64  # Increased for better training
  num_workers: 8
  shuffle: true
  cache: true
  split_train: "train"  # Use full training set (50,000 images)
  split_eval: "test"

model:
  in_ch: 3
  latent_hw: 32
  ch: 96                # Increased from 64 to ~8M params
  ch_mult: [1, 2, 4]    # Channel multipliers: [96, 192, 384]
  num_res_blocks: 2     # Residual blocks per level
  num_classes: 10

time:
  schedule: linear      # a_t=1-t, b_t=t
  sampler: uniform_pair

train:
  epochs: 50            # More epochs for larger model
  lr: 3.0e-4
  wd: 0.0
  ema: 0.9999
  grad_clip: 1.0
  cfg_drop: 0.1
  seed: 0

eval:
  nfe: 1
  cfg_scale: 2.0
  log_every: 100
  ckpt_every: 5         # Save checkpoint every 5 epochs
